{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odqz9UQ_EPMr",
    "outputId": "10a0ca37-e8e5-47e8-b4b0-8989eb5ed8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EW-TJLpLg-sP",
    "outputId": "aaee6571-1f6d-46a4-ac52-9b98235e5af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy[cuda101]==2.3.5 in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (53.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (7.4.5)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: cupy-cuda101>=5.0.0b4; extra == \"cuda101\" in /usr/local/lib/python3.6/dist-packages (from spacy[cuda101]==2.3.5) (7.4.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]==2.3.5) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]==2.3.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]==2.3.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda101]==2.3.5) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[cuda101]==2.3.5) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda101>=5.0.0b4; extra == \"cuda101\"->spacy[cuda101]==2.3.5) (0.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda101>=5.0.0b4; extra == \"cuda101\"->spacy[cuda101]==2.3.5) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[cuda101]==2.3.5) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[cuda101]==2.3.5) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install cupy==7.8.0\n",
    "# ! pip install Cython==0.28\n",
    "\n",
    "! pip install -U spacy[cuda101]==2.3.5\n",
    "# ! pip install preshed\n",
    "\n",
    "# ! pip uninstall spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XVCplGxiYQ4",
    "outputId": "1fbec217-5ea5-45de-ece4-89d1c45833ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.5\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)\n",
    "gpu = spacy.require_gpu()\n",
    "print('GPU:', gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "eQdlDwjrEeg9"
   },
   "outputs": [],
   "source": [
    "import plac\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "training_data = []\n",
    "\n",
    "def main(input_file=None, output_file=None):\n",
    "    try:\n",
    "        \n",
    "        lines=[]\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "    \n",
    "        with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "main(\"gdrive/MyDrive/Content/ner_corpus_260.json\", \"gdrive/MyDrive/Content/json_to_spacy.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZamJ5BLTK9I",
    "outputId": "f98b4a92-2728-4fb4-fd5b-e4d2e71eda9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gpu_usage.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpu_usage.sh\n",
    "#! /bin/bash\n",
    "#comment: run for 10 seconds, change it as per your use\n",
    "end=$((SECONDS+10))\n",
    "\n",
    "while [ $SECONDS -lt $end ]; do\n",
    "    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu >> gpu.log\n",
    "    #comment: or use below command and comment above using #\n",
    "    #nvidia-smi dmon -i 0 -s mu -d 1 -o TD >> gpu.log\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lajpXK0TWyN",
    "outputId": "c37ffa59-ce36-4540-e58f-f468ea86889b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 2 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%bash --bg\n",
    "\n",
    "bash gpu_usage.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsG-BcywHJdd",
    "outputId": "8230ee95-c1fb-4986-d7b8-de0e0430415e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Training begun\n",
      "1048575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with an empty lexeme normalization table. This may degrade the performance to some degree. If this is intentional or this language doesn't have a normalization table, please ignore this warning.\n",
      "  **kwargs\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/language.py:639: UserWarning: [W034] Please install the package spacy-lookups-data in order to include the default lexeme normalization table for the language 'en'.\n",
      "  **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "{'ner': 104680.25584310446}\n",
      "********************\n",
      "{'ner': 94191.99758931434}\n",
      "********************\n",
      "{'ner': 90920.22853269053}\n",
      "********************\n",
      "{'ner': 89082.45212023689}\n",
      "********************\n",
      "{'ner': 87856.99479971964}\n",
      "********************\n",
      "{'ner': 86931.5795251533}\n",
      "********************\n",
      "{'ner': 86058.55712457711}\n",
      "********************\n",
      "{'ner': 85787.25580748363}\n",
      "********************\n",
      "{'ner': 85184.41469945524}\n",
      "********************\n",
      "{'ner': 84880.47813810405}\n",
      "********************\n",
      "{'ner': 84467.49875361068}\n",
      "********************\n",
      "{'ner': 84127.74693301113}\n",
      "********************\n",
      "{'ner': 83935.50906732537}\n",
      "********************\n",
      "{'ner': 83543.31740390894}\n",
      "********************\n",
      "{'ner': 83303.04972089014}\n",
      "********************\n",
      "{'ner': 83099.46528229535}\n",
      "********************\n",
      "{'ner': 82990.55022321713}\n",
      "********************\n",
      "{'ner': 82942.89307262153}\n",
      "********************\n",
      "{'ner': 82506.54251249382}\n",
      "********************\n",
      "{'ner': 82453.95765366348}\n",
      "********************\n",
      "{'ner': 82308.54327418406}\n",
      "********************\n",
      "{'ner': 82058.33520999968}\n",
      "********************\n",
      "{'ner': 82285.12036753041}\n",
      "********************\n",
      "{'ner': 82191.73712743982}\n",
      "********************\n",
      "{'ner': 82174.86461383685}\n",
      "Entities in 'Peter Strzok, the F.B.I. agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations, has been fired.'\n",
      "B-per - Peter\n",
      "Saved model to ner_out\n",
      "Loading from ner_out\n",
      "B-per - Peter\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "# Loading training data \n",
    "with open ('gdrive/MyDrive/Content/json_to_spacy.spacy', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "#     new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "#     n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, new_model_name='new_model', output_dir=None,\n",
    "         n_iter=25):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "        reset_weigths = False \n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "        reset_weights = True\n",
    "    \n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None or reset_weights:\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training begun\")\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()     \n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    print(len(TRAIN_DATA))\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "\n",
    "            batches = minibatch(TRAIN_DATA, \n",
    "                            size=compounding(4., 128., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch) \n",
    "            # Updating the weights\n",
    "                nlp.update(texts, annotations, sgd=optimizer, \n",
    "                       drop=0.35, losses=losses)\n",
    "                # print(losses)\n",
    "        \n",
    "            print(\"*\"*20)\n",
    "            print(losses)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = \"Peter Strzok, the F.B.I. agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations, has been fired.\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_,'-', ent.text)\n",
    "\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # Test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_,'-', ent.text)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)\n",
    "\n",
    "main(model=None,\n",
    "    new_model_name=\"ner_new\",\n",
    "    output_dir=\"ner_out\",\n",
    "    n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PItyYiz0PiAg",
    "outputId": "71bb1cd3-f9b2-4fc0-b50f-31ce50b9ddd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.5\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NER using Spacy_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
